{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing the HuggingFace Libraries ","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers[torch] datasets","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:28:20.883970Z","iopub.execute_input":"2024-04-29T20:28:20.884203Z","iopub.status.idle":"2024-04-29T20:28:34.009750Z","shell.execute_reply.started":"2024-04-29T20:28:20.884179Z","shell.execute_reply":"2024-04-29T20:28:34.008518Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Loading the CNN_DAILYMAIL Dataset ","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ncnn_dailymail = load_dataset(\"cnn_dailymail\", \"3.0.0\", split='validation')","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:28:34.015231Z","iopub.execute_input":"2024-04-29T20:28:34.015509Z","iopub.status.idle":"2024-04-29T20:28:53.440696Z","shell.execute_reply.started":"2024-04-29T20:28:34.015478Z","shell.execute_reply":"2024-04-29T20:28:53.439805Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/15.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1250a38bd1e14ec0bbce4bc30649dbd8"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 257M/257M [00:01<00:00, 221MB/s]  \nDownloading data: 100%|██████████| 257M/257M [00:01<00:00, 239MB/s]  \nDownloading data: 100%|██████████| 259M/259M [00:01<00:00, 239MB/s]  \nDownloading data: 100%|██████████| 34.7M/34.7M [00:00<00:00, 148MB/s] \nDownloading data: 100%|██████████| 30.0M/30.0M [00:00<00:00, 138MB/s] \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f312d08a2b7740c8b6bfdcb9ebf4c636"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0974b975e705447f8d9bd5d1fa5ada80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6202d961be204b80ac66e2e5e43aaf4b"}},"metadata":{}}]},{"cell_type":"markdown","source":"Looking at the number of rows and columns of the dataset","metadata":{}},{"cell_type":"code","source":"cnn_dailymail","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:28:53.443588Z","iopub.execute_input":"2024-04-29T20:28:53.444142Z","iopub.status.idle":"2024-04-29T20:28:53.450446Z","shell.execute_reply.started":"2024-04-29T20:28:53.444112Z","shell.execute_reply":"2024-04-29T20:28:53.449627Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['article', 'highlights', 'id'],\n    num_rows: 13368\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"Splitting the dataset into training and testing set","metadata":{}},{"cell_type":"code","source":"cnn_dailymail = cnn_dailymail.train_test_split(test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:28:53.451757Z","iopub.execute_input":"2024-04-29T20:28:53.452139Z","iopub.status.idle":"2024-04-29T20:28:53.478987Z","shell.execute_reply.started":"2024-04-29T20:28:53.452107Z","shell.execute_reply":"2024-04-29T20:28:53.478152Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"cnn_dailymail","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:28:53.480056Z","iopub.execute_input":"2024-04-29T20:28:53.480307Z","iopub.status.idle":"2024-04-29T20:28:53.485810Z","shell.execute_reply.started":"2024-04-29T20:28:53.480285Z","shell.execute_reply":"2024-04-29T20:28:53.484937Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 10694\n    })\n    test: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 2674\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"Checking if the dataset is loaded correctly","metadata":{}},{"cell_type":"code","source":"example = cnn_dailymail[\"train\"][0]\nfor key in example:\n    print(\"A key of the example: \\\"{}\\\"\".format(key))\n    print(\"The value corresponding to the key-\\\"{}\\\"\\n \\\"{}\\\"\".format(key, example[key]))","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:28:53.486983Z","iopub.execute_input":"2024-04-29T20:28:53.487249Z","iopub.status.idle":"2024-04-29T20:28:53.498123Z","shell.execute_reply.started":"2024-04-29T20:28:53.487218Z","shell.execute_reply":"2024-04-29T20:28:53.497263Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"A key of the example: \"article\"\nThe value corresponding to the key-\"article\"\n \"A former university professor who taught a class on the hit CBS show Survivor and entered the reality competition this year was was voted off the program in the fourth episode. Max Dawson, who has done extensive research on the show and spent two years readying himself to compete, was voted off on Wednesday because other contestants thought he was 'annoying'. Dawson taught a class called 'The Tribe Has Spoken: Surviving TV's New Reality' at Northwestern University in 2012 and 2013. Scroll down for video . Max Dawson, a former Northwestern University professor who taught a class called 'The Tribe Has Spoken: Surviving TV's New Reality' for two years was voted off the hit reality show Survivor after just four episodes . Contestants apparently thought Dawson was 'annoying'. He was seen over-strategizing and spewing knowledge of past seasons throughout his time on the show . But his constant over-strategizing, interpersonal skills and vast knowledge of past seasons seemed to bother his competitors, according to the Chicago Tribune. Before the voting ceremony on Wednesday's episode, contestant Will Sims said that the debate as to who would go home was 'really about who is the most annoying', and Dawson was offed. When host Jeff Probst announced during the ceremony that it was the time to use a hidden immunity idol, Dawson, who received his Bachelor's degree from Brown University in 1999 and went on to get a PhD from Northwestern in 2008, spoke up. 'Hey Jeff, hold up, bro,' He said as reached behind his back, coming up with nothing. 'I just always wanted to say that.' His fellow contestants rolled their eyes at his fan-like comment. Dawson, a member of the White Collar Tribe while on the show, said that he found 'the personal dynamics' to be a difficult aspect of playing the show . This season of Survivor contestants were divided into three teams: the Blue Collar Tribe, White Collar Tribe and No Collar Tribe. Though Dawson was only ever a member of the white tribe, there is cross-tribe mingling on the show. Dawson, who now lives and works in Los Angeles, told the Tribune before the show aired that he 'had no idea how difficult it would be to actually play'. 'And that applied to the the basic survival elements - the food, the water, the shelter, the sun - but it really, more than anything for me, to the personal dynamics,' he said. After the episode aired on Wednesday, Dawson claimed on Twitter that he didn't know that much Survivor trivia, despite teaching a class on the show for two years. 'Full disclosure: I actually don't know that much Survivor trivia. It's just most that players who call themselves fans are actually casuals,' he wrote. After Wednesday's episode aired Dawson, claimed on Twitter that he didn't know that much Survivor trivia, despite teaching a class on the show for two years, and called out contestants who said they were fans, but didn't know much about the show .\"\nA key of the example: \"highlights\"\nThe value corresponding to the key-\"highlights\"\n \"Max Dawson taught a class called 'The Tribe Has Spoken: Surviving TV's New Reality' at Northwestern University in 2012 and 2013 .\nHe was seen rattling off past season trivia and over-strategizing on show .\nDawson said one of the hardest parts of the show was handling 'personal dynamics'\"\nA key of the example: \"id\"\nThe value corresponding to the key-\"id\"\n \"e5bfede9305edba8677d8ffe2b6ba617cde22463\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Preprocessing and Tokenization","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"t5-small\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:28:53.499364Z","iopub.execute_input":"2024-04-29T20:28:53.499687Z","iopub.status.idle":"2024-04-29T20:28:59.911013Z","shell.execute_reply.started":"2024-04-29T20:28:53.499657Z","shell.execute_reply":"2024-04-29T20:28:59.909835Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"713046aed5174711845b44295be9da39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"617c8b76b6214b6584ba4ff52f02626f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34f2daaf11954d339ef65fc9d35e1124"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_text = tokenizer(example['article'])\nfor key in tokenized_text:\n    print(key)\n    print(tokenized_text[key])","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:28:59.912493Z","iopub.execute_input":"2024-04-29T20:28:59.912964Z","iopub.status.idle":"2024-04-29T20:28:59.923714Z","shell.execute_reply.started":"2024-04-29T20:28:59.912938Z","shell.execute_reply":"2024-04-29T20:28:59.922444Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (704 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"input_ids\n[71, 1798, 3819, 5812, 113, 4436, 3, 9, 853, 30, 8, 1560, 19856, 504, 3, 31400, 11, 5136, 8, 2669, 2259, 48, 215, 47, 47, 3, 11060, 326, 8, 478, 16, 8, 4509, 5640, 5, 5370, 31676, 6, 113, 65, 612, 3616, 585, 30, 8, 504, 11, 1869, 192, 203, 1065, 53, 2448, 12, 5978, 6, 47, 3, 11060, 326, 30, 2875, 250, 119, 4233, 2366, 816, 3, 88, 47, 3, 31, 24889, 8149, 31, 5, 31676, 4436, 3, 9, 853, 718, 3, 31, 634, 2702, 346, 4498, 8927, 2217, 10, 3705, 7003, 53, 1424, 31, 7, 368, 23963, 31, 44, 30198, 636, 16, 1673, 11, 6386, 25731, 323, 21, 671, 3, 5, 5370, 31676, 6, 3, 9, 1798, 30198, 636, 5812, 113, 4436, 3, 9, 853, 718, 3, 31, 634, 2702, 346, 4498, 8927, 2217, 10, 3705, 7003, 53, 1424, 31, 7, 368, 23963, 31, 21, 192, 203, 47, 3, 11060, 326, 8, 1560, 2669, 504, 3, 31400, 227, 131, 662, 13562, 3, 5, 21537, 2366, 8743, 816, 31676, 47, 3, 31, 24889, 8149, 31, 5, 216, 47, 894, 147, 18, 7, 17, 2206, 122, 2610, 11, 15142, 3108, 1103, 13, 657, 9385, 1019, 112, 97, 30, 8, 504, 3, 5, 299, 112, 3917, 147, 18, 7, 17, 2206, 122, 2610, 6, 28978, 1098, 11, 4248, 1103, 13, 657, 9385, 3776, 12, 13965, 112, 9216, 6, 1315, 12, 8, 3715, 30013, 5, 3103, 8, 10601, 7252, 30, 2875, 31, 7, 5640, 6, 4233, 288, 2003, 6619, 7, 243, 24, 8, 5054, 38, 12, 113, 133, 281, 234, 47, 3, 31, 60, 1427, 81, 113, 19, 8, 167, 16241, 31, 6, 11, 31676, 47, 13, 19565, 5, 366, 2290, 8507, 8570, 28626, 2162, 383, 8, 7252, 24, 34, 47, 8, 97, 12, 169, 3, 9, 5697, 26510, 21979, 6, 31676, 6, 113, 1204, 112, 10199, 31, 7, 1952, 45, 3899, 636, 16, 5247, 11, 877, 30, 12, 129, 3, 9, 10360, 45, 30198, 16, 2628, 6, 5468, 95, 5, 3, 31, 3845, 63, 8507, 6, 1520, 95, 6, 9161, 6, 31, 216, 243, 38, 3495, 1187, 112, 223, 6, 1107, 95, 28, 1327, 5, 3, 31, 196, 131, 373, 1114, 12, 497, 24, 5, 31, 978, 4999, 4233, 2366, 3, 10671, 70, 2053, 44, 112, 1819, 18, 2376, 1670, 5, 31676, 6, 3, 9, 1144, 13, 8, 1945, 9919, 291, 2702, 346, 298, 30, 8, 504, 6, 243, 24, 3, 88, 435, 3, 31, 532, 525, 14966, 31, 12, 36, 3, 9, 1256, 2663, 13, 1556, 8, 504, 3, 5, 100, 774, 13, 3, 31400, 4233, 2366, 130, 8807, 139, 386, 2323, 10, 8, 2419, 9919, 291, 2702, 346, 6, 1945, 9919, 291, 2702, 346, 11, 465, 9919, 291, 2702, 346, 5, 4229, 31676, 47, 163, 664, 3, 9, 1144, 13, 8, 872, 14430, 6, 132, 19, 2269, 18, 1788, 346, 3, 51, 53, 697, 30, 8, 504, 5, 31676, 6, 113, 230, 1342, 11, 930, 16, 3144, 4975, 6, 1219, 8, 30013, 274, 8, 504, 3, 2378, 26, 24, 3, 88, 3, 31, 8399, 150, 800, 149, 1256, 34, 133, 36, 12, 700, 577, 31, 5, 3, 31, 7175, 24, 2930, 12, 8, 8, 1857, 9990, 2479, 3, 18, 8, 542, 6, 8, 387, 6, 8, 8596, 6, 8, 1997, 3, 18, 68, 34, 310, 6, 72, 145, 959, 21, 140, 6, 12, 8, 525, 14966, 6, 31, 3, 88, 243, 5, 621, 8, 5640, 3, 2378, 26, 30, 2875, 6, 31676, 7760, 30, 3046, 24, 3, 88, 737, 31, 17, 214, 24, 231, 3, 31400, 22377, 6, 3, 3565, 2119, 3, 9, 853, 30, 8, 504, 21, 192, 203, 5, 3, 31, 371, 83, 40, 12508, 10, 27, 700, 278, 31, 17, 214, 24, 231, 3, 31400, 22377, 5, 94, 31, 7, 131, 167, 24, 1508, 113, 580, 1452, 2675, 33, 700, 6995, 7, 6, 31, 3, 88, 2832, 5, 621, 2875, 31, 7, 5640, 3, 2378, 26, 31676, 6, 7760, 30, 3046, 24, 3, 88, 737, 31, 17, 214, 24, 231, 3, 31400, 22377, 6, 3, 3565, 2119, 3, 9, 853, 30, 8, 504, 21, 192, 203, 6, 11, 718, 91, 4233, 2366, 113, 243, 79, 130, 2675, 6, 68, 737, 31, 17, 214, 231, 81, 8, 504, 3, 5, 1]\nattention_mask\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocess_function(examples):\n    # Prepends the string \"summarize: \" to each document in the 'text' field of the input examples.\n    # This is done to instruct the T5 model on the task it needs to perform, which in this case is summarization.\n    inputs = [\"summarize: \" + doc for doc in examples[\"article\"]]\n\n    # Tokenizes the prepended input texts to convert them into a format that can be fed into the T5 model.\n    # Sets a maximum token length of 1024, and truncates any text longer than this limit.\n    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n\n    # Tokenizes the 'summary' field of the input examples to prepare the target labels for the summarization task.\n    # Sets a maximum token length of 128, and truncates any text longer than this limit.\n    labels = tokenizer(text_target=examples[\"highlights\"], max_length=128, truncation=True)\n\n    # Assigns the tokenized labels to the 'labels' field of model_inputs.\n    # The 'labels' field is used during training to calculate the loss and guide model learning.\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n\n    # Returns the prepared inputs and labels as a single dictionary, ready for training.\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:28:59.927063Z","iopub.execute_input":"2024-04-29T20:28:59.927339Z","iopub.status.idle":"2024-04-29T20:29:00.798219Z","shell.execute_reply.started":"2024-04-29T20:28:59.927315Z","shell.execute_reply":"2024-04-29T20:29:00.797278Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"tokenized_cnn_dailymail = cnn_dailymail.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:29:00.799648Z","iopub.execute_input":"2024-04-29T20:29:00.800035Z","iopub.status.idle":"2024-04-29T20:29:24.021513Z","shell.execute_reply.started":"2024-04-29T20:29:00.799995Z","shell.execute_reply":"2024-04-29T20:29:24.020600Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10694 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"440edabc69b741159b2aef07c109e511"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2674 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4f30d0c1b2642fa95aa78df82930ac9"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_cnn_dailymail['test'][0]['article']","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:29:24.022736Z","iopub.execute_input":"2024-04-29T20:29:24.023027Z","iopub.status.idle":"2024-04-29T20:29:24.030926Z","shell.execute_reply.started":"2024-04-29T20:29:24.023001Z","shell.execute_reply":"2024-04-29T20:29:24.029912Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"\"Couple: Becky Tait, 20, stepped in front of a train on the same stretch of tracks where her boyfriend Phil James, 19, did the same two months earlier . A heartbroken girlfriend killed herself at the same spot where her boyfriend committed suicide weeks earlier after struggling to come to terms with his death, an inquest heard today. Becky Tait, 20, stepped in front of a speeding train on the same tracks where her boyfriend Phil James, 19, had stood two months earlier. The care assistant from Stoke-on-Trent had been distraught after his death and was struggling to cope with her loss, Cannock Coroner’s Court in Staffordshire was told. Miss Tait went to the tracks where she died last November - where her ‘soul mate’ Mr James had died in September. Witnesses said she looked on ‘purposefully’ at the passenger train, which braked but struck her - causing multiple injuries. She was pronounced dead at the scene. Miss Tait had already tried to kill herself a month after Mr James’s death by taking an overdose, the inquest was told. She was kept in hospital for five days where she told mental health nurses she was unable to cope with the death of her boyfriend. Miss Tait was discharged in October and put into the care of her grandparents, but she killed herself one month later.\\xa0Margaret Jones, deputy assistant coroner for Staffordshire South, recorded a verdict of suicide. She said: ‘Becky Tait was a very young lady who was clearly traumatised by the death of her boyfriend. She was a very unhappy lady. ‘She clearly had been since the death of her boyfriend some weeks ago in similar circumstances. She had made some progress since being discharged from hospital.' 'Very unhappy': Miss Tait (above) had been distraught after her boyfriend's death and was struggling to cope . She said that on the 9th of November she was went to the railway tracks and was seen by witnesses. She added: 'This was a tragic event and her death deeply affected a lot of her family and friends.’ A young witness, who cannot be named, said in a statement read out to the court: ‘This girl was hanging over the barriers and I just thought she was in a rush or something. ‘I could see that she squeezed through the barriers and onto the train lines. I thought she was in a rush and she was just going to walk through the train lines. ‘The train came closer and closer, but she stood in the middle of the tracks. She waited for the train to get too close and then it hit her.’ Police Constable Melanie Dodd, of British Transport Police, told the inquest that Miss Tait had a suitcase at the time, which contained clothes and personal items. She added: ‘She moved on the tracks in front of the train, indicating a deliberate act. I am aware that her boyfriend did something similar weeks before.’ Family: Miss Tait's sister (left, with red hair) and mother (blonde, wearing scarf) were among the attendees at her inquest. They are pictured outside Cannock Coroner's Court today . Flowers left at the scene of the double-tragedy in Staffordshire . Miss Tait, who was born in Darlington, County Durham, had been going out with Mr James for a number of months and had planned a holiday to Portugal. Mr James was described as a huge fan of Robbie Williams and had a tattoo of the pop star’s name across his chest. The Stoke-born singer even took to his personal blog to pay tribute to Mr James, saying: ‘Save me a pint in heaven youth. Gone too soon. We'll all miss you. Much love, Rob.’ Consultant clinical psychologist Jurai Daromgkamas said Miss Tait was treated by the crisis team at South Staffordshire and Shropshire NHS Trust. She added: ‘The plan was to keep her safe and well until she was no longer feeling the suicide feelings. She was given some suggestions for coping strategies and was also told she was going to be monitored in the community. Tribute:\\xa0Mr James was described as a huge fan of Robbie Williams (pictured) and had a tattoo of the pop star’s name across his chest . Tribute from Williams: The Stoke-born singer even took to his personal blog to pay tribute to Mr James, saying: ‘Save me a pint in heaven youth. Gone too soon. We'll all miss you. Much love, Rob' ‘She said she no longer had suicide thoughts and hence she was discharged from the ward.’ Later, Miss Tait’s grandmother Val James said she was in Spain at the time of the death - and had received a text from the care worker just before she took her own life. She said: ‘The text said she can't cope anymore and that it would be okay and that she loved me. She was such a beautiful and fantastic girl. She was our world. We have done so much to remember her the way she was.’ Her friend Jade Johnson, 21, added: ‘She was usually the life of the party. She was the sort of person who would make sure people were enjoying themselves. Another friend, Debra Blythe, 50, said: ‘She was such a lovely person and she had so many friends. She would do anything to cheer people up, including a funny dance. That's just who she was.’ For confidential support call the Samaritans in the UK on 08457 90 90 90, visit a local Samaritans branch or click here for details .\""},"metadata":{}}]},{"cell_type":"code","source":"tokenized_cnn_dailymail['test'][0]['highlights']","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:29:24.032161Z","iopub.execute_input":"2024-04-29T20:29:24.032470Z","iopub.status.idle":"2024-04-29T20:29:24.073201Z","shell.execute_reply.started":"2024-04-29T20:29:24.032447Z","shell.execute_reply":"2024-04-29T20:29:24.072361Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'Becky Tait, of Stoke, killed herself at same spot in Staffordshire .\\nShe committed suicide two months after death of boyfriend Phil James .\\nCare assistant struggled to cope with loss and was hit by  train .'"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=\"t5-small\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:29:24.074433Z","iopub.execute_input":"2024-04-29T20:29:24.074965Z","iopub.status.idle":"2024-04-29T20:29:34.587923Z","shell.execute_reply.started":"2024-04-29T20:29:24.074933Z","shell.execute_reply":"2024-04-29T20:29:34.586988Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"2024-04-29 20:29:26.571196: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-29 20:29:26.571296: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-29 20:29:26.710896: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Finetuning the model using Rouge Evaluation Metric","metadata":{}},{"cell_type":"code","source":"! pip install -q evaluate rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:29:34.588978Z","iopub.execute_input":"2024-04-29T20:29:34.589527Z","iopub.status.idle":"2024-04-29T20:29:49.809660Z","shell.execute_reply.started":"2024-04-29T20:29:34.589499Z","shell.execute_reply":"2024-04-29T20:29:49.808462Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\n\nrouge = evaluate.load(\"rouge\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:29:49.811283Z","iopub.execute_input":"2024-04-29T20:29:49.811599Z","iopub.status.idle":"2024-04-29T20:29:52.270839Z","shell.execute_reply.started":"2024-04-29T20:29:49.811570Z","shell.execute_reply":"2024-04-29T20:29:52.270091Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fe0d38a33a14ec69b390f719a76fef9"}},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\n\ndef compute_metrics(eval_pred):\n    # Unpacks the evaluation predictions tuple into predictions and labels.\n    predictions, labels = eval_pred\n\n    # Decodes the tokenized predictions back to text, skipping any special tokens (e.g., padding tokens).\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n\n    # Replaces any -100 values in labels with the tokenizer's pad_token_id.\n    # This is done because -100 is often used to ignore certain tokens when calculating the loss during training.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n\n    # Decodes the tokenized labels back to text, skipping any special tokens (e.g., padding tokens).\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Computes the ROUGE metric between the decoded predictions and decoded labels.\n    # The use_stemmer parameter enables stemming, which reduces words to their root form before comparison.\n    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n\n    # Calculates the length of each prediction by counting the non-padding tokens.\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n\n    # Computes the mean length of the predictions and adds it to the result dictionary under the key \"gen_len\".\n    result[\"gen_len\"] = np.mean(prediction_lens)\n\n    # Rounds each value in the result dictionary to 4 decimal places for cleaner output, and returns the result.\n    return {k: round(v, 4) for k, v in result.items()}\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:29:52.271962Z","iopub.execute_input":"2024-04-29T20:29:52.272247Z","iopub.status.idle":"2024-04-29T20:29:52.279851Z","shell.execute_reply.started":"2024-04-29T20:29:52.272223Z","shell.execute_reply":"2024-04-29T20:29:52.278811Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, T5Config, T5Model, T5ForConditionalGeneration","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:29:52.281170Z","iopub.execute_input":"2024-04-29T20:29:52.281507Z","iopub.status.idle":"2024-04-29T20:29:52.325471Z","shell.execute_reply.started":"2024-04-29T20:29:52.281475Z","shell.execute_reply":"2024-04-29T20:29:52.324809Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"model_name = \"T5-small\"\n# Define the configuration with your desired number of attention heads\nconfig = T5Config.from_pretrained(model_name)\nconfig.num_heads = 2  # Change the number of attention heads to 2\nconfig.d_kv = config.d_model // config.num_heads\n\n# Now, use this modified configuration when initializing your model for fine-tuning\n# Initialize a new T5 model with the modified configuration\nmodel = T5ForConditionalGeneration(config=config)\n\n# Load the pretrained weights into the newly initialized model\nmodel.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:29:52.326379Z","iopub.execute_input":"2024-04-29T20:29:52.326650Z","iopub.status.idle":"2024-04-29T20:29:56.177146Z","shell.execute_reply.started":"2024-04-29T20:29:52.326627Z","shell.execute_reply":"2024-04-29T20:29:56.176215Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec9e691bb6354d68a42668a19fb90b40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74ea653807c24bb3991091f994406d89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59e24aef41954ed19cefcc030c723181"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"T5ForConditionalGeneration(\n  (shared): Embedding(32128, 512)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 512)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 8)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-5): 5 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 512)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 8)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-5): 5 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"print(config)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:29:56.178292Z","iopub.execute_input":"2024-04-29T20:29:56.178575Z","iopub.status.idle":"2024-04-29T20:29:56.184926Z","shell.execute_reply.started":"2024-04-29T20:29:56.178550Z","shell.execute_reply":"2024-04-29T20:29:56.183938Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"T5Config {\n  \"architectures\": [\n    \"T5ForConditionalGeneration\"\n  ],\n  \"classifier_dropout\": 0.0,\n  \"d_ff\": 2048,\n  \"d_kv\": 256,\n  \"d_model\": 512,\n  \"decoder_start_token_id\": 0,\n  \"dense_act_fn\": \"relu\",\n  \"dropout_rate\": 0.1,\n  \"eos_token_id\": 1,\n  \"feed_forward_proj\": \"relu\",\n  \"initializer_factor\": 1.0,\n  \"is_encoder_decoder\": true,\n  \"is_gated_act\": false,\n  \"layer_norm_epsilon\": 1e-06,\n  \"model_type\": \"t5\",\n  \"n_positions\": 512,\n  \"num_decoder_layers\": 6,\n  \"num_heads\": 2,\n  \"num_layers\": 6,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"relative_attention_max_distance\": 128,\n  \"relative_attention_num_buckets\": 32,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"early_stopping\": true,\n      \"length_penalty\": 2.0,\n      \"max_length\": 200,\n      \"min_length\": 30,\n      \"no_repeat_ngram_size\": 3,\n      \"num_beams\": 4,\n      \"prefix\": \"summarize: \"\n    },\n    \"translation_en_to_de\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to German: \"\n    },\n    \"translation_en_to_fr\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to French: \"\n    },\n    \"translation_en_to_ro\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to Romanian: \"\n    }\n  },\n  \"transformers_version\": \"4.39.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 32128\n}\n\n","output_type":"stream"}]},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"fine_tuned_t5_small_cnn_dailymail_model\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    weight_decay=0.01,\n    save_total_limit=3,\n    num_train_epochs=4,\n    predict_with_generate=True,\n    fp16=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:29:56.186154Z","iopub.execute_input":"2024-04-29T20:29:56.186476Z","iopub.status.idle":"2024-04-29T20:29:56.273345Z","shell.execute_reply.started":"2024-04-29T20:29:56.186447Z","shell.execute_reply":"2024-04-29T20:29:56.272380Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_cnn_dailymail[\"train\"],\n    eval_dataset=tokenized_cnn_dailymail[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:29:56.274513Z","iopub.execute_input":"2024-04-29T20:29:56.274787Z","iopub.status.idle":"2024-04-29T20:29:57.105687Z","shell.execute_reply.started":"2024-04-29T20:29:56.274762Z","shell.execute_reply":"2024-04-29T20:29:57.104921Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:29:57.108437Z","iopub.execute_input":"2024-04-29T20:29:57.108721Z","iopub.status.idle":"2024-04-29T21:07:52.339850Z","shell.execute_reply.started":"2024-04-29T20:29:57.108697Z","shell.execute_reply":"2024-04-29T21:07:52.339039Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240429_203018-6kbe33zo</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/northeastern-1/huggingface/runs/6kbe33zo' target=\"_blank\">major-salad-10</a></strong> to <a href='https://wandb.ai/northeastern-1/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/northeastern-1/huggingface' target=\"_blank\">https://wandb.ai/northeastern-1/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/northeastern-1/huggingface/runs/6kbe33zo' target=\"_blank\">https://wandb.ai/northeastern-1/huggingface/runs/6kbe33zo</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2676' max='2676' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2676/2676 37:15, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Rougelsum</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>8.189400</td>\n      <td>7.732235</td>\n      <td>0.076100</td>\n      <td>0.000100</td>\n      <td>0.068600</td>\n      <td>0.068500</td>\n      <td>19.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>7.806400</td>\n      <td>7.527384</td>\n      <td>0.067400</td>\n      <td>0.000500</td>\n      <td>0.058300</td>\n      <td>0.058200</td>\n      <td>19.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>7.559000</td>\n      <td>7.413552</td>\n      <td>0.069100</td>\n      <td>0.000600</td>\n      <td>0.058400</td>\n      <td>0.058400</td>\n      <td>19.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>7.498200</td>\n      <td>7.375935</td>\n      <td>0.067200</td>\n      <td>0.000500</td>\n      <td>0.057600</td>\n      <td>0.057600</td>\n      <td>19.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2676, training_loss=7.7254573896148635, metrics={'train_runtime': 2274.9035, 'train_samples_per_second': 18.803, 'train_steps_per_second': 1.176, 'total_flos': 1.1577810177490944e+16, 'train_loss': 7.7254573896148635, 'epoch': 4.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.save_model(\"fine_tuned_t5_small_cnn_dailymail_model\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T21:07:52.341124Z","iopub.execute_input":"2024-04-29T21:07:52.341393Z","iopub.status.idle":"2024-04-29T21:07:52.848964Z","shell.execute_reply.started":"2024-04-29T21:07:52.341368Z","shell.execute_reply":"2024-04-29T21:07:52.847859Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Inference Using Finetuned Model","metadata":{}},{"cell_type":"code","source":"text = cnn_dailymail['test'][100]['article']\ntext = \"summarize: \" + text\ntext","metadata":{"execution":{"iopub.status.busy":"2024-04-29T21:07:52.850343Z","iopub.execute_input":"2024-04-29T21:07:52.850709Z","iopub.status.idle":"2024-04-29T21:07:52.859544Z","shell.execute_reply.started":"2024-04-29T21:07:52.850675Z","shell.execute_reply":"2024-04-29T21:07:52.858299Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"\"summarize: UK defence cuts risk leaving America to tackle the twin threat of Russia and ISIS on its own, it has been claimed. Retired British army chief General Richard Dannatt suggested there was ‘no-one else’ for the US to turn to in Europe. It comes days after a UK think tank predicted that up to 30,000 British service personnel could go leaving the armed forces with a combined strength of just 115,000\\xa0by the end of the decade. Retired British army chief General Richard Dannatt (pictured) suggested there was ‘no-one else’ for the US to turn to in Europe . Now there are fears the United States could be left without a credible partner as it stands up to a number of global security threats. Lord Dannatt, who served as Chief of the General Staff between 2006 and 2009, told the Washington Post: ‘If the UK can’t do it, who else is the US going to turn to in Europe? There’s no one else.’ He added: ‘The concern is that we’re going to fall from being a significant player to a bit-part player. ‘The UK isn’t of much use to the US if we don’t have a worthwhile military force behind us. Anybody can talk tough. But if you don’t back it up, everyone just laughs at you.’ The comments come a week after the US Army chief of staff General Ray Odierno revealed he was ‘very concerned’ about cuts in the British military. A UK think tank predicted that up to 30,000 British service personnel could go leaving the armed forces with a combined strength of just 115,000 by the end of the decade (file picture) Earlier this week, the US ambassador to the United Nations, Samantha Power, warned of a ‘dangerous’ gulf emerging between American and European spending on defence. She told BBC Radio 4’s Today programme that governments in Europe needed to spend more and that cuts around the continent were ‘concerning’. A defence think tank has warned that up to 30,000 members of Britain’s forces face redundancy over the next parliament. It comes as Prime Minister David Cameron faces calls to pledge that funding for the military will be kept above the Nato target of at least 2 per cent of GDP. The 2013-14 defence budget was £34.3billion, down from £35.9billion in 2010-11, when the coalition took over. It comes after a report by the Royal United Services Institute (Rusi) said it was inevitable that Britain's defence spending would drop below the Nato target in the face of continuing austerity cuts.\""},"metadata":{}}]},{"cell_type":"code","source":"from transformers import pipeline\n\nsummarizer = pipeline(\"summarization\", model=\"fine_tuned_t5_small_cnn_dailymail_model\")\npred = summarizer(text)\npred","metadata":{"execution":{"iopub.status.busy":"2024-04-29T21:07:52.860492Z","iopub.execute_input":"2024-04-29T21:07:52.860728Z","iopub.status.idle":"2024-04-29T21:07:55.273534Z","shell.execute_reply.started":"2024-04-29T21:07:52.860707Z","shell.execute_reply":"2024-04-29T21:07:55.272512Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (569 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"[{'summary_text': \"The a 's a has been in the . It is he has been 't's in ' ' and '\"}]"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"fine_tuned_t5_small_cnn_dailymail_model\")\ninputs = tokenizer(text, return_tensors=\"pt\").input_ids\ninputs","metadata":{"execution":{"iopub.status.busy":"2024-04-29T21:07:55.275315Z","iopub.execute_input":"2024-04-29T21:07:55.276062Z","iopub.status.idle":"2024-04-29T21:07:55.353345Z","shell.execute_reply.started":"2024-04-29T21:07:55.276021Z","shell.execute_reply":"2024-04-29T21:07:55.352249Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (567 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"tensor([[21603,    10,  1270, 13613,  8620,  1020,  3140,  1371,    12,  8000,\n             8,  7390,  5888,    13,  4623,    11,    27, 14408,    30,   165,\n           293,     6,    34,    65,   118,  7760,     5,   419, 11809,    26,\n          2390,  9102,  5752,  2146,  4117,  7680,   144,    17,  5259,   132,\n            47,   458,    29,    32,    18,   782,  1307,    22,    21,     8,\n           837,    12,   919,    12,    16,  1740,     5,    94,   639,   477,\n           227,     3,     9,  1270,   317,  5040, 15439,    24,    95,    12,\n             3, 17093,  2390,   313,  4231,   228,   281,  3140,     8,     3,\n          8715,  3859,    28,     3,     9,  3334,  2793,    13,   131,   850,\n          5898,    57,     8,   414,    13,     8,  5112,     5,   419, 11809,\n            26,  2390,  9102,  5752,  2146,  4117,  7680,   144,    17,    41,\n         22665,    61,  5259,   132,    47,   458,    29,    32,    18,   782,\n          1307,    22,    21,     8,   837,    12,   919,    12,    16,  1740,\n             3,     5,   852,   132,    33, 14935,     8,   907,  1323,   228,\n            36,   646,   406,     3,     9, 25337,  2397,    38,    34,  5024,\n            95,    12,     3,     9,   381,    13,  1252,  1034, 11262,     5,\n          2809,  7680,   144,    17,     6,   113,  2098,    38,  5116,    13,\n             8,  2146, 10071,   344,  3581,    11,  2464,     6,  1219,     8,\n          2386,  1844,    10,   458,  5801,     8,  1270,    54,    22,    17,\n           103,    34,     6,   113,  1307,    19,     8,   837,   352,    12,\n           919,    12,    16,  1740,    58,   290,    22,     7,   150,    80,\n          1307,     5,    22,   216,   974,    10,   458,   634,  2410,    19,\n            24,    62,    22,    60,   352,    12,  1590,    45,   271,     3,\n             9,  1516,  1959,    12,     3,     9,   720,    18,  2274,  1959,\n             5,   458,   634,  1270,    19,    29,    22,    17,    13,   231,\n           169,    12,     8,   837,     3,    99,    62,   278,    22,    17,\n            43,     3,     9, 20167,  2716,  2054,  1187,   178,     5,  2372,\n          6965,    54,  1350,  3429,     5,   299,     3,    99,    25,   278,\n            22,    17,   223,    34,    95,     6,   921,   131,  8719,     7,\n            44,    25,     5,    22,    37,  2622,   369,     3,     9,   471,\n           227,     8,   837,  6788,  5752,    13,   871,  2146,  8279,  9899,\n           972,    29,    32,  5111,     3,    88,    47,   458,  8461,  4376,\n            22,    81,  8620,    16,     8,  2390,  2716,     5,    71,  1270,\n           317,  5040, 15439,    24,    95,    12,     3, 17093,  2390,   313,\n          4231,   228,   281,  3140,     8,     3,  8715,  3859,    28,     3,\n             9,  3334,  2793,    13,   131,   850,  5898,    57,     8,   414,\n            13,     8,  5112,    41, 11966,  1554,    61,     3, 15944,    48,\n           471,     6,     8,   837, 20850,    12,     8,   907,  9638,     6,\n         29298,  2621,     6, 15240,    13,     3,     9,   458,    26,  3280,\n          8283,    22,     3,  6106,    89,  7821,   344,   797,    11,  1611,\n          2887,    30, 13613,     5,   451,  1219,  9938,  5061,   314,    22,\n             7,  1960,  2486,    24, 10524,    16,  1740,   906,    12,  1492,\n            72,    11,    24,  8620,   300,     8, 10829,   130,   458, 11620,\n            52,    29,    53,    22,     5,    71, 13613,   317,  5040,    65,\n         15240,    24,    95,    12,     3, 17093,   724,    13,  7190,    22,\n             7,  3859,   522,  1131,  1106,  6833,   147,     8,   416, 20417,\n             5,    94,   639,    38,  5923,  3271,  1955, 18501,  8519,  3088,\n            12, 15387,    24,  3135,    21,     8,  2716,    56,    36,  2697,\n           756,     8,  9267,    32,  2387,    13,    44,   709,   204,   399,\n          3151,    13, 11284,     5,    37,  2038, 11590, 13613,  1487,    47,\n         23395, 21841,   115, 14916,     6,   323,    45, 23395,  9125,  1298,\n           115, 14916,    16,  2735,  9169,     6,   116,     8, 17952,   808,\n           147,     5,    94,   639,   227,     3,     9,   934,    57,     8,\n          3671,   907,  1799,  2548,    41, 17137,     7,    23,    61,   243,\n            34,    47, 17508,    24,  7190,    31,     7, 13613,  2887,   133,\n          2328,   666,     8,  9267,    32,  2387,    16,     8,   522,    13,\n          6168,   403,   449,   485,  8620,     5,     1]])"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"fine_tuned_t5_small_cnn_dailymail_model\")\noutputs = model.generate(inputs, max_new_tokens=100, do_sample=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T21:07:55.360204Z","iopub.execute_input":"2024-04-29T21:07:55.360548Z","iopub.status.idle":"2024-04-29T21:07:57.367726Z","shell.execute_reply.started":"2024-04-29T21:07:55.360512Z","shell.execute_reply":"2024-04-29T21:07:57.366578Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"tokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T21:07:57.368996Z","iopub.execute_input":"2024-04-29T21:07:57.369295Z","iopub.status.idle":"2024-04-29T21:07:57.379694Z","shell.execute_reply.started":"2024-04-29T21:07:57.369269Z","shell.execute_reply":"2024-04-29T21:07:57.378613Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"\"The a a a a a a a a a a a a a a a a a a a a a's's's's's's's's's's's's's's's's '\""},"metadata":{}}]},{"cell_type":"code","source":"pred[0]['summary_text']","metadata":{"execution":{"iopub.status.busy":"2024-04-29T21:07:57.382666Z","iopub.execute_input":"2024-04-29T21:07:57.383037Z","iopub.status.idle":"2024-04-29T21:07:57.390363Z","shell.execute_reply.started":"2024-04-29T21:07:57.383006Z","shell.execute_reply":"2024-04-29T21:07:57.389213Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"\"The a 's a has been in the . It is he has been 't's in ' ' and '\""},"metadata":{}}]},{"cell_type":"code","source":"preds = [pred[0]['summary_text']]","metadata":{"execution":{"iopub.status.busy":"2024-04-29T21:07:57.391614Z","iopub.execute_input":"2024-04-29T21:07:57.391958Z","iopub.status.idle":"2024-04-29T21:07:57.400837Z","shell.execute_reply.started":"2024-04-29T21:07:57.391927Z","shell.execute_reply":"2024-04-29T21:07:57.399896Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"labels = [cnn_dailymail['test'][100]['highlights']]","metadata":{"execution":{"iopub.status.busy":"2024-04-29T21:07:57.402079Z","iopub.execute_input":"2024-04-29T21:07:57.402489Z","iopub.status.idle":"2024-04-29T21:07:57.412926Z","shell.execute_reply.started":"2024-04-29T21:07:57.402465Z","shell.execute_reply":"2024-04-29T21:07:57.411792Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"rouge.compute(predictions=preds, references=labels, use_stemmer=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T21:07:57.414333Z","iopub.execute_input":"2024-04-29T21:07:57.414917Z","iopub.status.idle":"2024-04-29T21:07:57.628958Z","shell.execute_reply.started":"2024-04-29T21:07:57.414862Z","shell.execute_reply":"2024-04-29T21:07:57.627895Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"{'rouge1': 0.14925373134328357,\n 'rouge2': 0.030769230769230767,\n 'rougeL': 0.08955223880597014,\n 'rougeLsum': 0.14925373134328357}"},"metadata":{}}]}]}